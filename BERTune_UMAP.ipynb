{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drob-xx/Tune_BERTopic_HDBSCAN/blob/main/BERTune_UMAP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdZGZdrOO_lb"
      },
      "source": [
        "## **Introduction**\n",
        "\n",
        "> What is the relationship between [HDBSCAN](https://hdbscan.readthedocs.io/en/latest/index.html) settings and topic cluster creation in [BERTopic](https://maartengr.github.io/BERTopic/index.html)? This is the basic issue addressed below. In modeling a straight forward, homogeneous corpus of 30,000 English language news articles this notebook explains and shows dramatic improvements in the creation of a topic model. While the specific settings for this corpus will necessarily be different than for another corpus, the hope is that the discrete, reproduceable steps demonstrated here can be used to solve a wide-range of similar issues with many different corpi.\n",
        "\n",
        "## **The Corpus**\n",
        "\n",
        "> The [documents used here](https://www.kaggle.com/datasets/danrobinson707/newsdf) are from a [larger publicly available dataset](https://www.kaggle.com/datasets/harishcscode/all-news-articles-from-home-page-media-house) on Kaggle. The articles are a collection of news stories from a handful of major English language news publications. The predominant sources seem to be from the U.S., England and Australia. There is a mix of human interest, politics, science, medical, sports, entertainment and other typical, general audience subjects. The vast majority of articles are 500-1500 words in length. There is a long-tail of article sizes but only 0.84 percent (252) are more than 3000 words long. A small handful of the articles are in Welsh, not English.\n",
        "\n",
        "\n",
        "## **This Notebook**\n",
        "\n",
        "> This notebook is divided into four parts. \n",
        "\n",
        "* **Setup** installs, imports, defines some utility procedures, switches into the default dir, and reads two csv files into DataFrames. These are documents used throughout.\n",
        "\n",
        "* **BERT_ALL** creates a single BERTopic model from all 30,000 articles.\n",
        "\n",
        "* **BERT_1** and **BERT_2** are splits of the corpus **BERT_ALL** created because the results of experimenting with the parameters led the author to believe that a single parameterized HDBSCAN was insufficient for modeling this particular set of data. See below for details.\n",
        "\n",
        "## **The Scatterplots**\n",
        "\n",
        "> For each dataset there is a scatterplot that is a 2D TSNE reduction of the default BERTopic, 5D UMAP reduction of the underlying BERT embeddings. The scatterplots are a critical tool in evaluating the results of a particular configuration. Each datapoint represents one document and the hover text shows the first 400 characters of the modeled article. TSNE was used because it provides a somewhat more visually interpretable/coherent view of the data than UMAP provides in this case. The topic assignments are projected onto the scatterplot and the user can easily see a spatial relationship between  documents and categorizations. \n",
        "\n",
        "> These visualizations seem to invite close inspection as they provide a clearer and possibly unique view as to how the clustering effects document categorization. \n",
        "\n",
        "## **A Note On Randomness**\n",
        "\n",
        "> Of course many of the underlying algorithms are stochastic in nature. In practice during the preparation of this notebook multiple itterations of all of the techniques - either amalgamated within BERTopic or the independent components: UMAP, HDBSCAN, PacMAP, TSNE - were run many times. While small differences were observed from run to run they were minor and were not deemed relevant for the discussion at hand. The reader is encouraged to do their own investigations if they are curious about these issues.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "551jsKKJWt_P"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W5F-aB4Hzf10",
        "outputId": "51f5f4df-8e15-4687-b249-5318500079cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bertopic\n",
            "  Downloading bertopic-0.10.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.7/dist-packages (from bertopic) (5.5.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.7/dist-packages (from bertopic) (1.0.2)\n",
            "Requirement already satisfied: pyyaml<6.0 in /usr/local/lib/python3.7/dist-packages (from bertopic) (3.13)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from bertopic) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from bertopic) (1.21.6)\n",
            "Collecting hdbscan>=0.8.28\n",
            "  Downloading hdbscan-0.8.28.tar.gz (5.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.2 MB 8.0 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentence-transformers>=0.4.1\n",
            "  Downloading sentence-transformers-2.2.1.tar.gz (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting umap-learn>=0.5.0\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.7/dist-packages (from bertopic) (4.64.0)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.28->bertopic) (1.4.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.28->bertopic) (1.1.0)\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.28->bertopic) (0.29.30)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.5->bertopic) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.7.0->bertopic) (8.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly>=4.7.0->bertopic) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.1.0)\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 61.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1->bertopic) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.12.0+cu113)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1->bertopic) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 68.0 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.8.1\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 13.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.8.1->sentence-transformers>=0.4.1->bertopic) (4.11.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.8.1->sentence-transformers>=0.4.1->bertopic) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.8.1->sentence-transformers>=0.4.1->bertopic) (3.7.1)\n",
            "Collecting pyyaml<6.0\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 76.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.8.1->sentence-transformers>=0.4.1->bertopic) (4.1.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.8.1->sentence-transformers>=0.4.1->bertopic) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.8.1->sentence-transformers>=0.4.1->bertopic) (3.0.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 66.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.5.0->bertopic) (0.51.2)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.7.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 82.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (57.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.8.1->sentence-transformers>=0.4.1->bertopic) (3.8.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.8.1->sentence-transformers>=0.4.1->bertopic) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.8.1->sentence-transformers>=0.4.1->bertopic) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.8.1->sentence-transformers>=0.4.1->bertopic) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.8.1->sentence-transformers>=0.4.1->bertopic) (2.10)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (7.1.2)\n",
            "Building wheels for collected packages: hdbscan, sentence-transformers, umap-learn, pynndescent\n",
            "  Building wheel for hdbscan (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.28-cp37-cp37m-linux_x86_64.whl size=2342161 sha256=f8dfaedb02348c3b7fdf4d0a8f641817d3edac4cd367ca8e9929394904dd5c1b\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/7a/5e/259ccc841c085fc41b99ef4a71e896b62f5161f2bc8a14c97a\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.1-py3-none-any.whl size=125774 sha256=3b77d9895b98544486e48a080a72366c9c9a7261d932fdf14ea3e40bcdc4ae85\n",
            "  Stored in directory: /root/.cache/pip/wheels/58/27/2f/708b4f002c226e57b6243769da345c650633175c7634f93365\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=762e3b5d25ee574d36ae41261a1e34ce3c291c2ad9c7fb7386386dd07cfced02\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/52/a5/1fd9e3e76a7ab34f134c07469cd6f16e27ef3a37aeff1fe821\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.7-py3-none-any.whl size=54286 sha256=4fa999f3a8d44a91afa1a0491436d733e09873af1c1ffe8166fc9e28353adc97\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/2a/f8/7bd5dcec71bd5c669f6f574db3113513696b98f3f9b51f496c\n",
            "Successfully built hdbscan sentence-transformers umap-learn pynndescent\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers, sentencepiece, pynndescent, umap-learn, sentence-transformers, hdbscan, bertopic\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed bertopic-0.10.0 hdbscan-0.8.28 huggingface-hub-0.8.1 pynndescent-0.5.7 pyyaml-5.4.1 sentence-transformers-2.2.1 sentencepiece-0.1.96 tokenizers-0.12.1 transformers-4.20.1 umap-learn-0.5.3\n"
          ]
        }
      ],
      "source": [
        "!pip install bertopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVUn6Pei1iDc"
      },
      "outputs": [],
      "source": [
        "from bertopic import BERTopic\n",
        "# BERTopic installs both HDBSCAN and UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "\n",
        "from sklearn.feature_extraction import text \n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTElMQtc36vv"
      },
      "outputs": [],
      "source": [
        "# Utilitys functions\n",
        "\n",
        "def load(filepath):\n",
        "      with open(filepath, 'rb') as fp:\n",
        "          return pickle.load(fp)\n",
        "\n",
        "def save(var, filepath):\n",
        "      with open(filepath, 'wb') as fp:\n",
        "          return pickle.dump(var, fp)\n",
        "\n",
        "def PrepBERTopicTblForPlotly(targetTable, Text, BertModel, includeTopicText=True) :\n",
        "\n",
        "  targetTable['topic'] = [str(top) for top in BertModel._map_predictions(BertModel.hdbscan_model.labels_)]\n",
        "  \n",
        "  # There is a bug in BERTopic when the -1 category is not the largest category\n",
        "  sortedDF = BertModel.get_topic_info().sort_values(by=['Topic'])\n",
        "\n",
        "  topicDict = {key : '{} ({})'.format(val, num) for  key, val, num in zip(sortedDF['Topic'].values, sortedDF['Name'].values, sortedDF['Count'].values)}  \n",
        "  topicTexts = [topicDict[int(doctopic)] for doctopic in targetTable['topic'].values]\n",
        "  targetTable['topic_text'] = topicTexts\n",
        "  brtexts = []\n",
        "  topicnums = targetTable['topic']\n",
        "  for topicnum, texts in zip(topicnums, [txt[:400] for txt in Text]) :\n",
        "    if includeTopicText :\n",
        "      astr = '<br><br>' + topicDict[int(topicnum)] + '<br><br>'\n",
        "    else :\n",
        "      astr = '<br><br>'\n",
        "    for idx in range(0, 400, 60) :\n",
        "      astr += texts[idx:idx+60]\n",
        "      astr += '<br>'\n",
        "    brtexts.append(astr)         \n",
        "  targetTable['text'] = brtexts\n",
        "\n",
        "def getOrderedTopicTextFromBERT(BERTModel) :\n",
        "  # Get around a BERTopic Bug\n",
        "  sortedDF = BERTModel.get_topic_info().sort_values(by=['Topic'])\n",
        "\n",
        "  return ['{} ({})'.format(txt, cnt) for txt, cnt in zip(sortedDF['Name'].values, sortedDF['Count'].values)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR-lmFP50Tqt",
        "outputId": "fc3b481d-4532-46a9-955d-b4665dbbff28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Projects/BERTune_UMAP\n"
          ]
        }
      ],
      "source": [
        "# Change this to whereever you have downloaded the corpi \n",
        "#     and/or want to store your intermediate models\n",
        "\n",
        "cd /content/drive/MyDrive/Projects/BERTune_UMAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xWmPV5s2n2x"
      },
      "outputs": [],
      "source": [
        "News1DF = pd.read_csv('./News0.csv')\n",
        "News2DF = pd.read_csv('./News1.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRiNNCtbLaKi"
      },
      "source": [
        "## BERT_ALL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPJ-FdbjbDMx"
      },
      "outputs": [],
      "source": [
        "## Create a single corpi from the two - will become clearer below.\n",
        "\n",
        "ALLDF = pd.concat([News1DF, News2DF])\n",
        "ALLDF.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ms1EchRYLaKr"
      },
      "outputs": [],
      "source": [
        "# This cell runs in about 8 mins. on a Colab+ with GPU and addt'l memory. \n",
        "#   BERTopic is memory intensive, so watch out for crashes.\n",
        "\n",
        "\n",
        "# Adding these stop-words is cosmetic.\n",
        "stop_words = text.ENGLISH_STOP_WORDS.union(['said', 'say', 'says', 'year', 'years', 'new', 'mr'])\n",
        "vectorizer_model = CountVectorizer(ngram_range=(1, 3), stop_words=stop_words)\n",
        "\n",
        "# Chose a min_topic_size of 150 to force a reasonable clustering. The whole point\n",
        "#   of this exercise is to show that randomly setting these values (or just relying\n",
        "#   on defaults) will have a *huge* effect on the model.  In this case 150\n",
        "#   will force to a reasonably 'natural' segmentation. What is meant here\n",
        "#   by 'natural' and how this number was arrived at (through multiple iterations\n",
        "#   of HDBSCAN params), is beyond the scope of this notebook.\n",
        "\n",
        "BERT_ALL = BERTopic(\n",
        "                  vectorizer_model=vectorizer_model,\n",
        "                  calculate_probabilities=False,\n",
        "                  verbose=True,\n",
        "                  low_memory=True,\n",
        "                  min_topic_size=150                  \n",
        "                  )\n",
        "\n",
        "# Set UMAPs random state so that UMAP output will be consistent across runs\n",
        "\n",
        "BERT_ALL.umap_model.random_state=42\n",
        "\n",
        "# Fit the model\n",
        "\n",
        "BERT_ALL_Topics, _ = BERT_ALL.fit_transform(ALLDF['text'])\n",
        "\n",
        "# Save if you wish - takes about 2 mins.\n",
        "# BERT_ALL.save('./BERT_ALL')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z58E3J2ILaKs"
      },
      "outputs": [],
      "source": [
        "# If you saved...\n",
        "\n",
        "BERT_ALL = BERTopic.load('./BERT_ALL')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCGtf5FLLaKs"
      },
      "outputs": [],
      "source": [
        "# We need to create a 2D representation for the scatterplot. Could have used\n",
        "#    UMAP to do this - but partly because it would be a 2D reduction of a \n",
        "#    5D reduction of the original embeddings, it gets pretty visually sketchy.\n",
        "#    The reader is encouraged to play with different algorithms to \n",
        "#    aprehend the differences. There is no 'right' way to do this. TSNE in this \n",
        "#    case was convenient and salutary for this purpose. Check out PacMAP as well.\n",
        "\n",
        "BERT_ALL_TSNE = TSNE(n_components=2, learning_rate='auto',\n",
        "                  init='random').fit_transform(BERT_ALL.umap_model.embedding_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0AV72vZLaKs"
      },
      "outputs": [],
      "source": [
        "BERT_ALL_DF = pd.DataFrame()\n",
        "BERT_ALL_DF['x'] = BERT_ALL_TSNE[:,0]\n",
        "BERT_ALL_DF['y'] = BERT_ALL_TSNE[:,1]\n",
        "\n",
        "PrepBERTopicTblForPlotly(BERT_ALL_DF, ALLDF['text'], BERT_ALL)\n",
        "ordered_list = getOrderedTopicTextFromBERT(BERT_ALL)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Initial Results**\n",
        "\n",
        "> In the scatter plot created below we see that the corpus has been broken up into 6 identified topics and one small set of outliers. Note that the relative positioning of the documents within these clusters is very cohesive. \n",
        "\n",
        "> The most important feature of this particular configuration shows that topics 1-5 are sports related. Within those, the largest set seems to be predominantly soccer with a fair number of rugby articles. The other groupings are somewhat mixed but overall are about American Football, Golf, Tennis, Car Racing and Boxing. While these sports groupings are by no means homogeneous, they are remarkably weighted towards a particular sport.\n",
        "\n",
        "> The other, very large cluster can be thought of as \"News\" For the most part there are few sports stories in this grouping. Where they do intrude it seems to be because there is overlap with other kinds of news so you get Sports/Crime, Sports/Politics, Sports/Culture etc. etc. Far more interesting is that within this cluster it is easy to visually identify many many sub-clusters that presumably HDBSCAN, with the settings used, was unable to identify as unique. Yet by zooming around the cluster and examining specific large and small visually clustered groupings, it is apparent that there is internal structure - articles about earthquakes, crime, sexual-abuse, movies, internet companies, etc. etc. grouped around one another. While there *seem* to be some outliers, there are really very few. Furthermore, closer inspection of the entire article may reveal clues as to why these articles were positioned as they were.\n",
        "\n",
        "> After concerted and failing attempts to find parameters that would result in a better sub-categorization of the \"News\" cluster, an guess about the limits of HDBSCAN was made. Namely, that the geometry of this particular dataset means that no single HDBSCAN set of parameters can both preserve the basic Sports/News dichotomy while at the same time allow for a \"rationalized\" categorization of the News articles that would correspond to their seemingly coherent spatial positioning. Interestingly, other visualizations, namely using UMAP 2D and PacMAP visualizations of the embeddings more clearly shows a noticable gap between the Sports and News categorizatons.\n",
        "\n",
        "> Note that you can click and double click on the legend to hide / show individual or groups of categories."
      ],
      "metadata": {
        "id": "3rnPR8BhbmKR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ebomwYH7LaKt"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(BERT_ALL_DF, x='x', y='y', \n",
        "                color='topic_text', \n",
        "                width=1200, \n",
        "                height=850,\n",
        "                hover_data= {'x' : False,\n",
        "                            'y' : False,\n",
        "                            'topic' : False,\n",
        "                            'text' : True },\n",
        "                category_orders={'topic_text' : ordered_list})\n",
        "fig.update_layout(\n",
        "    hoverlabel=dict(\n",
        "        bgcolor=\"white\",\n",
        "))\n",
        "\n",
        "fig.update_traces(marker=dict(size=6),\n",
        "                  selector=dict(mode='markers')\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzFFioSXgqx5"
      },
      "source": [
        "## BERT_1 - 'NEWS'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The corpus was divided into two parts. Using settings similar to the above, the set of 'News' topics were separated from the 'Sports'. The BERTopic model was created and then a series of HDBSCAN parameters were cycled through. Based on this output a min_cluster_size of 330 and min_samples of 165 seemed like very good candidates. The process for selecting the optimum values is relatively complicated and won't be explained here. The author encourages questsions regarding why these parameters were chosen. These parameters are not being presented as fully optimized - but do suffice for the overall demonstration."
      ],
      "metadata": {
        "id": "QypDYVYBjtLQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7h9d5l7-0dd1"
      },
      "outputs": [],
      "source": [
        "stop_words = text.ENGLISH_STOP_WORDS.union(['said', 'say', 'says', 'year', 'years', 'new', 'mr'])\n",
        "vectorizer_model = CountVectorizer(ngram_range=(1, 3), stop_words=stop_words)\n",
        "hdbscan_model = HDBSCAN(min_samples=165,  \n",
        "                        min_cluster_size=330)\n",
        "\n",
        "BERT_1 = BERTopic(\n",
        "                  vectorizer_model=vectorizer_model,\n",
        "                  calculate_probabilities=False,\n",
        "                  verbose=True,\n",
        "                  low_memory=True,                  \n",
        "                  hdbscan_model=hdbscan_model,\n",
        "                  )\n",
        "# Set UMAPs random state so that UMAP output will be consistent across runs\n",
        "\n",
        "BERT_ALL.umap_model.random_state=42\n",
        "\n",
        "# Fit the model\n",
        "\n",
        "BERT_1_topics, _ = BERT_1.fit_transform(News1DF['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPyTvD3o4JGM"
      },
      "outputs": [],
      "source": [
        "BERT_1.save('./BERT_1')\n",
        "save(BERT_1_topics, './BERT_1_topics')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IM5nY08_EAf3"
      },
      "outputs": [],
      "source": [
        "BERT_1 = BERTopic.load('./BERT_1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISDn795Xb_0V"
      },
      "outputs": [],
      "source": [
        "stop_words = text.ENGLISH_STOP_WORDS.union(['said', 'say', 'says', 'year', 'years', 'new', 'mr'])\n",
        "vectorizer_model = CountVectorizer(ngram_range=(1, 3), stop_words=stop_words)\n",
        "BERT_1.update_topics(News1DF['text'], BERT_1_topics, vectorizer_model=vectorizer_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55nMOKXqeDCT"
      },
      "outputs": [],
      "source": [
        "BERT_1_TSNE = TSNE(n_components=2, learning_rate='auto',\n",
        "                  init='random').fit_transform(BERT_1.umap_model.embedding_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbV-ZRW5BhXS"
      },
      "outputs": [],
      "source": [
        "BERT_1_DF = pd.DataFrame()\n",
        "BERT_1_DF['x'] = BERT_1_TSNE[:,0]\n",
        "BERT_1_DF['y'] = BERT_1_TSNE[:,1]\n",
        "\n",
        "PrepBERTopicTblForPlotly(BERT_1_DF, News1DF['text'], BERT_1)\n",
        "ordered_list = getOrderedTopicTextFromBERT(BERT_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a nice segmentation of the texts. Uncategorized have shot up, but they are more or less evenly distributed. There are some areas where it seems like it would be 'nice' to have a better identified clustering, but overall this is a much better representation than the above. It may very well be that further experimentation with the clustering would further fine-tune these results."
      ],
      "metadata": {
        "id": "OHQZ9sBFnkKz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZgmnBvnDvr-"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(BERT_1_DF, x='x', y='y', \n",
        "                color='topic_text', \n",
        "                width=850, \n",
        "                height=600,\n",
        "                hover_data= {'x' : False,\n",
        "                            'y' : False,\n",
        "                            'topic' : False,\n",
        "                            'text' : True },\n",
        "                category_orders={'topic_text' : ordered_list})\n",
        "fig.update_layout(\n",
        "    hoverlabel=dict(\n",
        "        bgcolor=\"white\",\n",
        "))\n",
        "\n",
        "fig.update_traces(marker=dict(size=6),\n",
        "                  selector=dict(mode='markers')\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zym-cCGlhDsg"
      },
      "source": [
        "## BERT_2\n",
        "\n",
        "Using a min_samples of 40 and min_cluster_size of 80 (arrived at through the same process as the BERT_1 parameters) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6P99enPhhB7"
      },
      "outputs": [],
      "source": [
        "stop_words = text.ENGLISH_STOP_WORDS.union(['said', 'say', 'says', 'year', 'years', 'new', 'mr'])\n",
        "vectorizer_model = CountVectorizer(ngram_range=(1, 3), stop_words=stop_words)\n",
        "hdbscan_model = HDBSCAN(min_samples=40,  \n",
        "                        min_cluster_size=80)\n",
        "\n",
        "BERT_2 = BERTopic(\n",
        "                  vectorizer_model=vectorizer_model,\n",
        "                  calculate_probabilities=False,\n",
        "                  verbose=True,\n",
        "                  low_memory=True,                  \n",
        "                  hdbscan_model=hdbscan_model,\n",
        "                  )\n",
        "\n",
        "# Set UMAPs random state so that UMAP output will be consistent across runs\n",
        "\n",
        "BERT_ALL.umap_model.random_state=42\n",
        "\n",
        "# Fit the model\n",
        "\n",
        "BERT_2_Topics, _ = BERT_2.fit_transform(News2DF['text'])\n",
        "\n",
        "BERT_2.save('./BERT_2')\n",
        "save(BERT_2, './BERT_2')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8gBGIjvhhB8"
      },
      "outputs": [],
      "source": [
        "BERT_2 = BERTopic.load('./BERT_2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX7IgX80hhB-"
      },
      "outputs": [],
      "source": [
        "BERT_2_TSNE = TSNE(n_components=2, learning_rate='auto',\n",
        "                  init='random').fit_transform(BERT_2.umap_model.embedding_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMRzmz6ehhB-"
      },
      "outputs": [],
      "source": [
        "BERT_2_DF = pd.DataFrame()\n",
        "BERT_2_DF['x'] = BERT_2_TSNE[:,0]\n",
        "BERT_2_DF['y'] = BERT_2_TSNE[:,1]\n",
        "\n",
        "PrepBERTopicTblForPlotly(BERT_2_DF, News2DF['text'], BERT_2)\n",
        "ordered_list = getOrderedTopicTextFromBERT(BERT_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DFW_hYkhhB-"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(BERT_2_DF, x='x', y='y', \n",
        "                color='topic_text', \n",
        "                width=850, \n",
        "                height=600,\n",
        "                hover_data= {'x' : False,\n",
        "                            'y' : False,\n",
        "                            'topic' : False,\n",
        "                            'text' : True },\n",
        "                category_orders={'topic_text' : ordered_list})\n",
        "fig.update_layout(\n",
        "    hoverlabel=dict(\n",
        "        bgcolor=\"white\",\n",
        "))\n",
        "\n",
        "fig.update_traces(marker=dict(size=6),\n",
        "                  selector=dict(mode='markers')\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "BERTune_UMAP.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "551jsKKJWt_P"
      ],
      "mount_file_id": "1M-LYoI4pCGjK517pMiDfxkekW8XEqzIc",
      "authorship_tag": "ABX9TyP1xaP3gol+RNs2laNkx+fu",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}